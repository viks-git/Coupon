# Required Assignment 5.1 – Will the Customer Accept the Coupon?
Author: _Your Name Here_

This notebook explores which customers accept in‑vehicle coupons and why. It adheres to the rubric requirements for **Project Organization**, **Syntax and Code Quality**, **Visualizations**, and **Findings**.
## Assignment Overview

**Goal:** Use visual exploration and probability‑based summaries to answer: **Will a customer accept the coupon?**  
- Accepted = **Y = 1** (“Right away” or “Later, before the coupon expires”)  
- Not accepted = **Y = 0** (“No, I do not want the coupon”)  
- Coupon types: Less expensive restaurants (<$20), Coffee House, Carryout & Takeaway, Bar, More expensive restaurants ($20–$50).

**What you’ll deliver in this notebook:**
1. Clean, readable **EDA** that contrasts **accepted vs. not** accepted groups
2. Clear **plots** (Matplotlib/Seaborn) with descriptive titles and readable labels
3. A concise **Findings & Recommendations** section with actionable next steps
## 0. Setup & Imports
Please ensure the dataset (e.g., `coupons.csv`) is placed in `../data/`. The code below attempts to load the most common schema from the UCI in‑vehicle coupon dataset.

# Libraries: imported and aliased correctly
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# display options
pd.set_option('display.max_columns', 100)
pd.set_option('display.width', 120)

# plot defaults
# (Do not enforce specific colors; rely on defaults for readability)
sns.set(context="notebook", style="whitegrid")

## 1. Load Data

# Provide a couple of candidate filenames; change if needed.
data_path = r"C:\Users\vpandey\Desktop\ML Learning\exercises\assignment5_1_starter\data\coupons.csv"

df_raw = pd.read_csv(data_path)
df = df_raw.copy()

print("Loaded shape:", df.shape)
display(df.head(3))
display(pd.DataFrame({"column": df.columns}))

## 2. Basic Cleaning & Target Variable
The acceptance column is known to vary across copies (e.g., `Y`, `accepted`, `Accept`). The helper below tries to locate and standardize it.

# Try to locate the acceptance column robustly
possible_targets = ["Y", "y", "accept", "accepted", "Accept", "Accepted", "label"]
target_col = None
for c in df.columns:
    if c.strip() in possible_targets:
        target_col = c
        break

if target_col is None:
    # Heuristic: look for a binary column with values like {0,1} or {'Yes','No','Y','N'}
    for c in df.columns:
        vals = set(map(lambda x: str(x).strip().lower(), df[c].dropna().unique()))
        if vals.issubset({"0","1","y","n","yes","no","true","false"}):
            target_col = c
            break

if target_col is None:
    raise ValueError("Could not automatically detect the acceptance target column. Please set `target_col` manually.")

# Standardize target to 0/1 integer
def to_binary(x):
    x = str(x).strip().lower()
    if x in {"1", "y", "yes", "true", "accept", "accepted"}:
        return 1
    else:
        return 0

df["accepted_coupon"] = df[target_col].apply(to_binary).astype(int)
print("Target column mapped from:", target_col, "-> 'accepted_coupon' (0/1)")
df.drop(columns=[target_col], inplace=False)

# Optional: quick NA handling on common columns
na_counts = df.isna().sum().sort_values(ascending=False)
display(na_counts.head(10))

## 3. Overview & Descriptive Statistics

# Overall acceptance rate
acc_rate = df["accepted_coupon"].mean()
print(f"Overall acceptance rate: {acc_rate:.3f}")

# Basic describe; separate numeric & non-numeric
display(df.describe(include=[np.number]).T)
display(df.describe(include=['O']).T)

## 4. Visual Explorations
We’ll compare acceptance across categorical and behavioral features common to this dataset: `coupon` type, `destination`, `time`, `weather`, `passenger`, `Bar`, `CoffeeHouse`, `income`, `age`, etc.

# Helper plotting function
def plot_acceptance_by(feature, top_n=None, rotate_x=False):
    temp = (df.groupby(feature)["accepted_coupon"]
              .agg(['count','mean'])
              .rename(columns={'count':'n','mean':'accept_rate'})
              .sort_values('n', ascending=False))
    if top_n is not None:
        temp = temp.head(top_n)

    fig, ax1 = plt.subplots(figsize=(8, 5))
    temp["n"].plot(kind="bar", ax=ax1)
    ax1.set_ylabel("Count")
    ax1.set_xlabel(feature)
    ax1.set_title(f"Acceptance by {feature} – counts (bars) and rate (line)")

    ax2 = ax1.twinx()
    ax2.plot(range(len(temp)), temp["accept_rate"], marker="o")
    ax2.set_ylabel("Acceptance Rate")

    if rotate_x:
        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha="right")

    plt.tight_layout()
    plt.show()
    display(temp.reset_index())

# Candidate features (use only those present)
features_to_try = [
    "coupon", "destination", "time", "weather", "passenger", "temperature",
    "age", "maritalStatus", "income",
    "Bar", "CoffeeHouse", "CarryAway", "RestaurantLessThan20", "Restaurant20To50"
]

existing = [c for c in features_to_try if c in df.columns]
for feat in existing:
    try:
        plot_acceptance_by(feat, rotate_x=True)
    except Exception as e:
        print(f"Skipping {feat}: {e}")

### Subplots Example: Compare Multiple Coupon Types
Below we visualize acceptance rates by coupon type and another feature (e.g., time of day).

if "coupon" in df.columns and "time" in df.columns:
    # Build a pivot for acceptance rates
    pivot = (df.groupby(["coupon","time"])["accepted_coupon"].mean().unstack())
    pivot = pivot.loc[pivot.index]  # no-op; keeps order explicit
    fig, ax = plt.subplots(figsize=(9, 5))
    pivot.plot(kind="bar", ax=ax)
    ax.set_title("Acceptance Rate by Coupon & Time")
    ax.set_ylabel("Acceptance Rate")
    ax.set_xlabel("Coupon Type")
    plt.tight_layout()
    plt.show()
    display(pivot)

## 5. Inferential Statistics (Chi‑Square on Categorical Features)
We’ll test whether acceptance is independent of selected categorical features using a chi‑square test.

from scipy.stats import chi2_contingency

def chi_square_feature(feature):
    ct = pd.crosstab(df[feature], df["accepted_coupon"])
    chi2, p, dof, exp = chi2_contingency(ct)
    return {"feature": feature, "chi2": chi2, "p_value": p, "dof": dof, "table": ct}

cat_features = [c for c in existing if df[c].dtype == 'O' or df[c].dtype.name == 'category']
results = []
for c in cat_features:
    try:
        results.append(chi_square_feature(c))
    except Exception as e:
        print(f"Chi-square skipped for {c}: {e}")

chi_df = pd.DataFrame([{"feature": r["feature"], "chi2": r["chi2"], "p_value": r["p_value"], "dof": r["dof"]} for r in results])
chi_df = chi_df.sort_values("p_value")
display(chi_df.head(10))

## 6. Findings & Recommendations
This section generates a concise narrative based on descriptive stats and the top chi‑square results.

_This section satisfies the assignment requirement to clearly state findings, highlight actionable insights, and list next steps._

top_drivers = chi_df.nsmallest(5, "p_value") if not chi_df.empty else pd.DataFrame()
overall = df["accepted_coupon"].mean()

print("=== Summary (Auto‑Generated) ===")
print(f"Overall acceptance rate: {overall:.1%}")
if not top_drivers.empty:
    print("Top features associated with differences in acceptance (by chi‑square p‑value):")
    for _, row in top_drivers.iterrows():
        feat = row["feature"]
        # Show top categories by acceptance for quick actionability
        tmp = df.groupby(feat)["accepted_coupon"].mean().sort_values(ascending=False).head(3)
        print(f" • {feat}: top categories by acceptance -> {list(tmp.index)} (rates: {list((tmp*100).round(1))}%)")

print("\nActionable next steps:")
print("1) Target segments with higher acceptance (e.g., top categories above) for relevant coupon types.")
print("2) Review timing/context (e.g., time of day, passenger) to schedule offers when conversion is higher.")
print("3) If applicable, personalize by behavior (e.g., Bar/CoffeeHouse frequency) and income/age bands.")
print("4) Consider A/B tests to validate uplift for high‑propensity segments, using acceptance as the primary KPI.")

> **Note:** Replace _Your Name Here_ above and optionally add a short paragraph here summarizing your interpretation in your own words, referencing the visuals and stats produced.
